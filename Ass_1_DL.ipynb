{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass_1_DL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naitikbhise/repo1/blob/master/Ass_1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0aJxXSNqtgGq",
        "colab_type": "code",
        "outputId": "af819f3e-3f8f-4f4c-cfdf-e102071d919f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9da3e91-0cbb-4b15-9594-dae5194da650\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f9da3e91-0cbb-4b15-9594-dae5194da650\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ptb-lm.py to ptb-lm.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ptb-lm.py': b'#!/bin/python\\n# coding: utf-8\\n\\n# Code outline/scaffold for \\n# ASSIGNMENT 2: RNNs, Attention, and Optimization\\n# By Tegan Maharaj, David Krueger, and Chin-Wei Huang\\n# IFT6135 at University of Montreal\\n# Winter 2019\\n#\\n# based on code from:\\n#    https://github.com/deeplearningathome/pytorch-language-model/blob/master/reader.py\\n#    https://github.com/ceshine/examples/blob/master/word_language_model/main.py\\n#    https://github.com/teganmaharaj/zoneout/blob/master/zoneout_word_ptb.py\\n#    https://github.com/harvardnlp/annotated-transformer\\n\\n# GENERAL INSTRUCTIONS: \\n#    - ! IMPORTANT! \\n#      Unless we\\'re otherwise notified we will run exactly this code, importing \\n#      your models from models.py to test them. If you find it necessary to \\n#      modify or replace this script (e.g. if you are using TensorFlow), you \\n#      must justify this decision in your report, and contact the TAs as soon as \\n#      possible to let them know. You are free to modify/add to this script for \\n#      your own purposes (e.g. monitoring, plotting, further hyperparameter \\n#      tuning than what is required), but remember that unless we\\'re otherwise \\n#      notified we will run this code as it is given to you, NOT with your \\n#      modifications.\\n#    - We encourage you to read and understand this code; there are some notes \\n#      and comments to help you.\\n#    - Typically, all of your code to submit should be written in models.py; \\n#      see further instructions at the top of that file / in TODOs.\\n#          - RNN recurrent unit \\n#          - GRU recurrent unit\\n#          - Multi-head attention for the Transformer\\n#    - Other than this file and models.py, you will probably also write two \\n#      scripts. Include these and any other code you write in your git repo for \\n#      submission:\\n#          - Plotting (learning curves, loss w.r.t. time, gradients w.r.t. hiddens)\\n#          - Loading and running a saved model (computing gradients w.r.t. hiddens, \\n#            and for sampling from the model)\\n\\n# PROBLEM-SPECIFIC INSTRUCTIONS:   \\n#    - For Problems 1-3, paste the code for the RNN, GRU, and Multi-Head attention \\n#      respectively in your report, in a monospace font.\\n#    - For Problem 4.1 (model comparison), the hyperparameter settings you should run are as follows:\\n#          --model=RNN --optimizer=ADAM --initial_lr=0.0001 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35 --save_best\\n#          --model=GRU --optimizer=SGD_LR_SCHEDULE --initial_lr=10 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35 --save_best\\n#          --model=TRANSFORMER --optimizer=SGD_LR_SCHEDULE --initial_lr=20 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=6 --dp_keep_prob=0.9 --save_best\\n#    - In those experiments, you should expect to see approximately the following\\n#      perplexities:\\n#                  RNN: train:  120  val: 157\\n#                  GRU: train:   65  val: 104\\n#          TRANSFORMER:  train:  67  val: 146\\n#    - For Problem 4.2 (exploration of optimizers), you will make use of the \\n#      experiments from 4.1, and should additionally run the following experiments:\\n#          --model=RNN --optimizer=SGD --initial_lr=0.0001 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35 \\n#          --model=GRU --optimizer=SGD --initial_lr=10 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35\\n#          --model=TRANSFORMER --optimizer=SGD --initial_lr=20 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=6 --dp_keep_prob=.9\\n#          --model=RNN --optimizer=SGD_LR_SCHEDULE --initial_lr=1 --batch_size=20 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.35\\n#          --model=GRU --optimizer=ADAM --initial_lr=0.0001 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35\\n#          --model=TRANSFORMER --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=.9\\n#    - For Problem 4.3 (exloration of hyperparameters), do your best to get \\n#      better validation perplexities than the settings given for 4.1. You may \\n#      try any combination of the hyperparameters included as arguments in this \\n#      script\\'s ArgumentParser, but do not implement any additional \\n#      regularizers/features. You may (and will probably want to) run a lot of \\n#      different things for just 1-5 epochs when you are trying things out, but \\n#      you must report at least 3 experiments on each architecture that have run\\n#      for at least 40 epochs.\\n#    - For Problem 5, perform all computations / plots based on saved models \\n#      from Problem 4.1. NOTE this means you don\\'t have to save the models for \\n#      your exploration, which can make things go faster. (Of course\\n#      you can still save them if you like; just add the flag --save_best). \\n#    - For Problem 5.1, you can modify the loss computation in this script \\n#      (search for \"LOSS COMPUTATION\" to find the appropriate line. Remember to \\n#      submit your code.\\n#    - For Problem 5.3, you must implement the generate method of the RNN and \\n#      GRU.  Implementing this method is not considered part of problems 1/2 \\n#      respectively, and will be graded as part of Problem 5.3\\n\\n\\nimport argparse\\nimport time\\nimport collections\\nimport os\\nimport sys\\nimport torch\\nimport torch.nn\\nfrom torch.autograd import Variable\\nimport torch.nn as nn\\nimport numpy\\nnp = numpy\\n\\n# NOTE ==============================================\\n# This is where your models are imported\\nfrom models import RNN, GRU \\nfrom models import make_model as TRANSFORMER\\n\\n\\n##############################################################################\\n#\\n# ARG PARSING AND EXPERIMENT SETUP\\n#\\n##############################################################################\\n\\nparser = argparse.ArgumentParser(description=\\'PyTorch Penn Treebank Language Modeling\\')\\n\\n# Arguments you may need to set to run different experiments in 4.1 & 4.2.\\nparser.add_argument(\\'--data\\', type=str, default=\\'data\\',\\n                    help=\\'location of the data corpus. We suggest you change the default\\\\\\n                    here, rather than passing as an argument, to avoid long file paths.\\')\\nparser.add_argument(\\'--model\\', type=str, default=\\'GRU\\',\\n                    help=\\'type of recurrent net (RNN, GRU, TRANSFORMER)\\')\\nparser.add_argument(\\'--optimizer\\', type=str, default=\\'SGD_LR_SCHEDULE\\',\\n                    help=\\'optimization algo to use; SGD, SGD_LR_SCHEDULE, ADAM\\')\\nparser.add_argument(\\'--seq_len\\', type=int, default=35,\\n                    help=\\'number of timesteps over which BPTT is performed\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=20,\\n                    help=\\'size of one minibatch\\')\\nparser.add_argument(\\'--initial_lr\\', type=float, default=20.0,\\n                    help=\\'initial learning rate\\')\\nparser.add_argument(\\'--hidden_size\\', type=int, default=200,\\n                    help=\\'size of hidden layers. IMPORTANT: for the transformer\\\\\\n                    this must be a multiple of 16.\\')\\nparser.add_argument(\\'--save_best\\', action=\\'store_true\\',\\n                    help=\\'save the model for the best validation performance\\')\\nparser.add_argument(\\'--num_layers\\', type=int, default=2,\\n                    help=\\'number of hidden layers in RNN/GRU, or number of transformer blocks in TRANSFORMER\\')\\n\\n# Other hyperparameters you may want to tune in your exploration\\nparser.add_argument(\\'--emb_size\\', type=int, default=200,\\n                    help=\\'size of word embeddings\\')\\nparser.add_argument(\\'--num_epochs\\', type=int, default=40,\\n                    help=\\'number of epochs to stop after\\')\\nparser.add_argument(\\'--dp_keep_prob\\', type=float, default=0.35,\\n                    help=\\'dropout *keep* probability. drop_prob = 1-dp_keep_prob \\\\\\n                    (dp_keep_prob=1 means no dropout)\\')\\n\\n# Arguments that you may want to make use of / implement more code for\\nparser.add_argument(\\'--debug\\', action=\\'store_true\\') \\nparser.add_argument(\\'--save_dir\\', type=str, default=\\'\\',\\n                    help=\\'path to save the experimental config, logs, model \\\\\\n                    This is automatically generated based on the command line \\\\\\n                    arguments you pass and only needs to be set if you want a \\\\\\n                    custom dir name\\')\\nparser.add_argument(\\'--evaluate\\', action=\\'store_true\\',\\n                    help=\"use this flag to run on the test set. Only do this \\\\\\n                    ONCE for each model setting, and only after you\\'ve \\\\\\n                    completed ALL hyperparameter tuning on the validation set.\\\\\\n                    Note we are not requiring you to do this.\")\\n\\n# DO NOT CHANGE THIS (setting the random seed makes experiments deterministic, \\n# which helps for reproducibility)\\nparser.add_argument(\\'--seed\\', type=int, default=1111,\\n                    help=\\'random seed\\')\\n\\nargs = parser.parse_args()\\nargsdict = args.__dict__\\nargsdict[\\'code_file\\'] = sys.argv[0]\\n\\n# Use the model, optimizer, and the flags passed to the script to make the \\n# name for the experimental dir\\nprint(\"\\\\n########## Setting Up Experiment ######################\")\\nflags = [flag.lstrip(\\'--\\').replace(\\'/\\', \\'\\').replace(\\'\\\\\\\\\\', \\'\\') for flag in sys.argv[1:]]\\nexperiment_path = os.path.join(args.save_dir+\\'_\\'.join([argsdict[\\'model\\'],\\n                                         argsdict[\\'optimizer\\']] \\n                                         + flags))\\n\\n# Increment a counter so that previous results with the same args will not\\n# be overwritten. Comment out the next four lines if you only want to keep\\n# the most recent results.\\ni = 0\\nwhile os.path.exists(experiment_path + \"_\" + str(i)):\\n    i += 1\\nexperiment_path = experiment_path + \"_\" + str(i)\\n\\n# Creates an experimental directory and dumps all the args to a text file\\nos.mkdir(experiment_path)\\nprint (\"\\\\nPutting log in %s\"%experiment_path)\\nargsdict[\\'save_dir\\'] = experiment_path\\nwith open (os.path.join(experiment_path,\\'exp_config.txt\\'), \\'w\\') as f:\\n    for key in sorted(argsdict):\\n        f.write(key+\\'    \\'+str(argsdict[key])+\\'\\\\n\\')\\n\\n# Set the random seed manually for reproducibility.\\ntorch.manual_seed(args.seed)\\n\\n# Use the GPU if you have one\\nif torch.cuda.is_available():\\n    print(\"Using the GPU\")\\n    device = torch.device(\"cuda\") \\nelse:\\n    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\\\\n      of memory. \\\\n You can try setting batch_size=1 to reduce memory usage\")\\n    device = torch.device(\"cpu\")\\n\\n\\n###############################################################################\\n#\\n# LOADING & PROCESSING\\n#\\n###############################################################################\\n\\n# HELPER FUNCTIONS\\ndef _read_words(filename):\\n    with open(filename, \"r\") as f:\\n      return f.read().replace(\"\\\\n\", \"<eos>\").split()\\n\\ndef _build_vocab(filename):\\n    data = _read_words(filename)\\n\\n    counter = collections.Counter(data)\\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\\n\\n    words, _ = list(zip(*count_pairs))\\n    word_to_id = dict(zip(words, range(len(words))))\\n    id_to_word = dict((v, k) for k, v in word_to_id.items())\\n\\n    return word_to_id, id_to_word\\n\\ndef _file_to_word_ids(filename, word_to_id):\\n    data = _read_words(filename)\\n    return [word_to_id[word] for word in data if word in word_to_id]\\n\\n# Processes the raw data from text files\\ndef ptb_raw_data(data_path=None, prefix=\"ptb\"):\\n    train_path = os.path.join(data_path, prefix + \".train.txt\")\\n    valid_path = os.path.join(data_path, prefix + \".valid.txt\")\\n    test_path = os.path.join(data_path, prefix + \".test.txt\")\\n\\n    word_to_id, id_2_word = _build_vocab(train_path)\\n    train_data = _file_to_word_ids(train_path, word_to_id)\\n    valid_data = _file_to_word_ids(valid_path, word_to_id)\\n    test_data = _file_to_word_ids(test_path, word_to_id)\\n    return train_data, valid_data, test_data, word_to_id, id_2_word\\n\\n# Yields minibatches of data\\ndef ptb_iterator(raw_data, batch_size, num_steps):\\n    raw_data = np.array(raw_data, dtype=np.int32)\\n\\n    data_len = len(raw_data)\\n    batch_len = data_len // batch_size\\n    data = np.zeros([batch_size, batch_len], dtype=np.int32)\\n    for i in range(batch_size):\\n        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\\n\\n    epoch_size = (batch_len - 1) // num_steps\\n\\n    if epoch_size == 0:\\n        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\\n\\n    for i in range(epoch_size):\\n        x = data[:, i*num_steps:(i+1)*num_steps]\\n        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\\n        yield (x, y)\\n\\n\\nclass Batch:\\n    \"Data processing for the transformer. This class adds a mask to the data.\"\\n    def __init__(self, x, pad=-1):\\n        self.data = x\\n        self.mask = self.make_mask(self.data, pad)\\n    \\n    @staticmethod\\n    def make_mask(data, pad):\\n        \"Create a mask to hide future words.\"\\n\\n        def subsequent_mask(size):\\n            \"\"\" helper function for creating the masks. \"\"\"\\n            attn_shape = (1, size, size)\\n            subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\\'uint8\\')\\n            return torch.from_numpy(subsequent_mask) == 0\\n\\n        mask = (data != pad).unsqueeze(-2)\\n        mask = mask & Variable(\\n            subsequent_mask(data.size(-1)).type_as(mask.data))\\n        return mask\\n\\n\\n# LOAD DATA\\nprint(\\'Loading data from \\'+args.data)\\nraw_data = ptb_raw_data(data_path=args.data)\\ntrain_data, valid_data, test_data, word_to_id, id_2_word = raw_data\\nvocab_size = len(word_to_id)\\nprint(\\'  vocabulary size: {}\\'.format(vocab_size))\\n\\n\\n###############################################################################\\n# \\n# MODEL SETUP\\n#\\n###############################################################################\\n\\n# NOTE ==============================================\\n# This is where your model code will be called. You may modify this code\\n# if required for your implementation, but it should not typically be necessary,\\n# and you must let the TAs know if you do so.\\nif args.model == \\'RNN\\':\\n    model = RNN(emb_size=args.emb_size, hidden_size=args.hidden_size, \\n                seq_len=args.seq_len, batch_size=args.batch_size,\\n                vocab_size=vocab_size, num_layers=args.num_layers, \\n                dp_keep_prob=args.dp_keep_prob) \\nelif args.model == \\'GRU\\':\\n    model = GRU(emb_size=args.emb_size, hidden_size=args.hidden_size, \\n                seq_len=args.seq_len, batch_size=args.batch_size,\\n                vocab_size=vocab_size, num_layers=args.num_layers, \\n                dp_keep_prob=args.dp_keep_prob)\\nelif args.model == \\'TRANSFORMER\\':\\n    if args.debug:  # use a very small model\\n        model = TRANSFORMER(vocab_size=vocab_size, n_units=16, n_blocks=2)\\n    else:\\n        # Note that we\\'re using num_layers and hidden_size to mean slightly \\n        # different things here than in the RNNs.\\n        # Also, the Transformer also has other hyperparameters \\n        # (such as the number of attention heads) which can change it\\'s behavior.\\n        model = TRANSFORMER(vocab_size=vocab_size, n_units=args.hidden_size, \\n                            n_blocks=args.num_layers, dropout=1.-args.dp_keep_prob) \\n    # these 3 attributes don\\'t affect the Transformer\\'s computations; \\n    # they are only used in run_epoch\\n    model.batch_size=args.batch_size\\n    model.seq_len=args.seq_len\\n    model.vocab_size=vocab_size\\nelse:\\n  print(\"Model type not recognized.\")\\n\\nmodel = model.to(device)\\n\\n# LOSS FUNCTION\\nloss_fn = nn.CrossEntropyLoss()\\nif args.optimizer == \\'ADAM\\':\\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\\n\\n# LEARNING RATE SCHEDULE    \\nlr = args.initial_lr\\nlr_decay_base = 1 / 1.15\\nm_flat_lr = 14.0 # we will not touch lr for the first m_flat_lr epochs\\n\\n\\n###############################################################################\\n# \\n# DEFINE COMPUTATIONS FOR PROCESSING ONE EPOCH\\n#\\n###############################################################################\\n\\ndef repackage_hidden(h):\\n    \"\"\"\\n    Wraps hidden states in new Tensors, to detach them from their history.\\n    \\n    This prevents Pytorch from trying to backpropagate into previous input \\n    sequences when we use the final hidden states from one mini-batch as the \\n    initial hidden states for the next mini-batch.\\n    \\n    Using the final hidden states in this way makes sense when the elements of \\n    the mini-batches are actually successive subsequences in a set of longer sequences.\\n    This is the case with the way we\\'ve processed the Penn Treebank dataset.\\n    \"\"\"\\n    if isinstance(h, Variable):\\n        return h.detach_()\\n    else:\\n        return tuple(repackage_hidden(v) for v in h)\\n\\n\\ndef run_epoch(model, data, is_train=False, lr=1.0):\\n    \"\"\"\\n    One epoch of training/validation (depending on flag is_train).\\n    \"\"\"\\n    if is_train:\\n        model.train()\\n    else:\\n        model.eval()\\n    epoch_size = ((len(data) // model.batch_size) - 1) // model.seq_len\\n    start_time = time.time()\\n    if args.model != \\'TRANSFORMER\\':\\n        hidden = model.init_hidden()\\n        hidden = hidden.to(device)\\n    costs = 0.0\\n    iters = 0\\n    losses = []\\n\\n    # LOOP THROUGH MINIBATCHES\\n    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\\n        if args.model == \\'TRANSFORMER\\':\\n            batch = Batch(torch.from_numpy(x).long().to(device))\\n            model.zero_grad()\\n            outputs = model.forward(batch.data, batch.mask).transpose(1,0)\\n            #print (\"outputs.shape\", outputs.shape)\\n        else:\\n            inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\\n            model.zero_grad()\\n            hidden = repackage_hidden(hidden)\\n            outputs, hidden = model(inputs, hidden)\\n\\n        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\\n        tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\\n\\n        # LOSS COMPUTATION\\n        # This line currently averages across all the sequences in a mini-batch \\n        # and all time-steps of the sequences.\\n        # For problem 5.3, you will (instead) need to compute the average loss \\n        #at each time-step separately. \\n        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)\\n        costs += loss.data.item() * model.seq_len\\n        losses.append(costs)\\n        iters += model.seq_len\\n        if args.debug:\\n            print(step, loss)\\n        if is_train:  # Only update parameters if training \\n            loss.backward()\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\\n            if args.optimizer == \\'ADAM\\':\\n                optimizer.step()\\n            else: \\n                for p in model.parameters():\\n                    if p.grad is not None:\\n                        p.data.add_(-lr, p.grad.data)\\n            if step % (epoch_size // 10) == 10:\\n                print(\\'step: \\'+ str(step) + \\'\\\\t\\' \\\\\\n                    + \"loss (sum over all examples\\' seen this epoch):\" + str(costs) + \\'\\\\t\\' \\\\\\n                    + \\'speed (wps):\\' + str(iters * model.batch_size / (time.time() - start_time)))\\n    return np.exp(costs / iters), losses\\n\\n\\n\\n###############################################################################\\n#\\n# RUN MAIN LOOP (TRAIN AND VAL)\\n#\\n###############################################################################\\n\\nprint(\"\\\\n########## Running Main Loop ##########################\")\\ntrain_ppls = []\\ntrain_losses = []\\nval_ppls = []\\nval_losses = []\\nbest_val_so_far = np.inf\\ntimes = []\\n\\n# In debug mode, only run one epoch\\nif args.debug:\\n    num_epochs = 1 \\nelse:\\n    num_epochs = args.num_epochs\\n\\n# MAIN LOOP\\nfor epoch in range(num_epochs):\\n    t0 = time.time()\\n    print(\\'\\\\nEPOCH \\'+str(epoch)+\\' ------------------\\')\\n    if args.optimizer == \\'SGD_LR_SCHEDULE\\':\\n        lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\\n        lr = lr * lr_decay # decay lr if it is time\\n\\n    # RUN MODEL ON TRAINING DATA\\n    train_ppl, train_loss = run_epoch(model, train_data, True, lr)\\n\\n    # RUN MODEL ON VALIDATION DATA\\n    val_ppl, val_loss = run_epoch(model, valid_data)\\n\\n\\n    # SAVE MODEL IF IT\\'S THE BEST SO FAR\\n    if val_ppl < best_val_so_far:\\n        best_val_so_far = val_ppl\\n        if args.save_best:\\n            print(\"Saving model parameters to best_params.pt\")\\n            torch.save(model.state_dict(), os.path.join(args.save_dir, \\'best_params.pt\\'))\\n        # NOTE ==============================================\\n        # You will need to load these parameters into the same model\\n        # for a couple Problems: so that you can compute the gradient \\n        # of the loss w.r.t. hidden state as required in Problem 5.2\\n        # and to sample from the the model as required in Problem 5.3\\n        # We are not asking you to run on the test data, but if you \\n        # want to look at test performance you would load the saved\\n        # model and run on the test data with batch_size=1\\n\\n    # LOC RESULTS\\n    train_ppls.append(train_ppl)\\n    val_ppls.append(val_ppl)\\n    train_losses.extend(train_loss)\\n    val_losses.extend(val_loss)\\n    times.append(time.time() - t0)\\n    log_str = \\'epoch: \\' + str(epoch) + \\'\\\\t\\' \\\\\\n            + \\'train ppl: \\' + str(train_ppl) + \\'\\\\t\\' \\\\\\n            + \\'val ppl: \\' + str(val_ppl)  + \\'\\\\t\\' \\\\\\n            + \\'best val: \\' + str(best_val_so_far) + \\'\\\\t\\' \\\\\\n            + \\'time (s) spent in epoch: \\' + str(times[-1])\\n    print(log_str)\\n    with open (os.path.join(args.save_dir, \\'log.txt\\'), \\'a\\') as f_:\\n        f_.write(log_str+ \\'\\\\n\\')\\n\\n# SAVE LEARNING CURVES\\nlc_path = os.path.join(args.save_dir, \\'learning_curves.npy\\')\\nprint(\\'\\\\nDONE\\\\n\\\\nSaving learning curves to \\'+lc_path)\\nnp.save(lc_path, {\\'train_ppls\\':train_ppls, \\n                  \\'val_ppls\\':val_ppls, \\n                  \\'train_losses\\':train_losses,\\n                  \\'val_losses\\':val_losses})\\n# NOTE ==============================================\\n# To load these, run \\n# >>> x = np.load(lc_path)[()]\\n# You will need these values for plotting learning curves (Problem 4)\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "PxTGzMMoy9he",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python models.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8luHU5W0Rv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp *.txt ./data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZoUoJ8nmDN-",
        "colab_type": "code",
        "outputId": "2de58f38-06b1-4887-e47c-f44d52623d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2718
        }
      },
      "cell_type": "code",
      "source": [
        "!python ptb-lm.py --model=RNN --optimizer=ADAM --initial_lr=0.0001 --batch_size=20 --seq_len=35 --hidden_size=1500 --num_layers=2 --dp_keep_prob=0.35 --save_best"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "########## Setting Up Experiment ######################\n",
            "\n",
            "Putting log in RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0\n",
            "Using the GPU\n",
            "Loading data from data\n",
            "  vocabulary size: 10000\n",
            "\n",
            "########## Running Main Loop ##########################\n",
            "\n",
            "EPOCH 0 ------------------\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "step: 10\tloss (sum over all examples' seen this epoch):3581.575765609741\tspeed (wps):2066.6010607729463\n",
            "step: 142\tloss (sum over all examples' seen this epoch):36982.47910261154\tspeed (wps):2200.5209552678907\n",
            "step: 274\tloss (sum over all examples' seen this epoch):67668.04701089859\tspeed (wps):2206.120135292009\n",
            "step: 406\tloss (sum over all examples' seen this epoch):97794.87948417664\tspeed (wps):2208.090027545625\n",
            "step: 538\tloss (sum over all examples' seen this epoch):127838.65808725357\tspeed (wps):2208.95341432109\n",
            "step: 670\tloss (sum over all examples' seen this epoch):157724.36531543732\tspeed (wps):2209.18536329775\n",
            "step: 802\tloss (sum over all examples' seen this epoch):187066.19871377945\tspeed (wps):2209.8186650415914\n",
            "step: 934\tloss (sum over all examples' seen this epoch):216361.5518426895\tspeed (wps):2210.3461444735044\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):245552.70827770233\tspeed (wps):2210.5561248081212\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):274457.43273973465\tspeed (wps):2210.5408931233646\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 0\ttrain ppl: 672.656321039687\tval ppl: 613.6192184612067\tbest val: 613.6192184612067\ttime (s) spent in epoch: 428.76144003868103\n",
            "\n",
            "EPOCH 1 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2436.623408794403\tspeed (wps):2176.4905940446934\n",
            "step: 142\tloss (sum over all examples' seen this epoch):30756.74812555313\tspeed (wps):2207.9425887785756\n",
            "step: 274\tloss (sum over all examples' seen this epoch):59510.77506303787\tspeed (wps):2209.005220312933\n",
            "step: 406\tloss (sum over all examples' seen this epoch):87975.92940092087\tspeed (wps):2210.407827300731\n",
            "step: 538\tloss (sum over all examples' seen this epoch):116537.28315353394\tspeed (wps):2210.56554266746\n",
            "step: 670\tloss (sum over all examples' seen this epoch):145082.1563386917\tspeed (wps):2211.299037393036\n",
            "step: 802\tloss (sum over all examples' seen this epoch):173209.92825508118\tspeed (wps):2211.260347079532\n",
            "step: 934\tloss (sum over all examples' seen this epoch):201384.87081050873\tspeed (wps):2211.178135441131\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):229558.23347330093\tspeed (wps):2211.137704360498\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):257502.7681350708\tspeed (wps):2211.199127803426\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 1\ttrain ppl: 458.3709451646685\tval ppl: 519.8641123018948\tbest val: 519.8641123018948\ttime (s) spent in epoch: 428.64650654792786\n",
            "\n",
            "EPOCH 2 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2377.6676869392395\tspeed (wps):2171.8999098520208\n",
            "step: 142\tloss (sum over all examples' seen this epoch):29968.213572502136\tspeed (wps):2205.8389866134653\n",
            "step: 274\tloss (sum over all examples' seen this epoch):58039.140763282776\tspeed (wps):2210.5393236858963\n",
            "step: 406\tloss (sum over all examples' seen this epoch):85828.60300064087\tspeed (wps):2210.5045145372164\n",
            "step: 538\tloss (sum over all examples' seen this epoch):113720.96720933914\tspeed (wps):2210.641818964858\n",
            "step: 670\tloss (sum over all examples' seen this epoch):141613.69978427887\tspeed (wps):2210.6116227098505\n",
            "step: 802\tloss (sum over all examples' seen this epoch):169077.08469867706\tspeed (wps):2210.9436467553132\n",
            "step: 934\tloss (sum over all examples' seen this epoch):196650.70593118668\tspeed (wps):2211.2401785341267\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):224224.4739294052\tspeed (wps):2211.714494898006\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):251568.41704130173\tspeed (wps):2212.2804408364823\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 2\ttrain ppl: 398.642354642663\tval ppl: 440.29825235163537\tbest val: 440.29825235163537\ttime (s) spent in epoch: 428.3514976501465\n",
            "\n",
            "EPOCH 3 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2348.9973378181458\tspeed (wps):2168.066353174697\n",
            "step: 142\tloss (sum over all examples' seen this epoch):29444.98579263687\tspeed (wps):2209.47028974332\n",
            "step: 274\tloss (sum over all examples' seen this epoch):57053.840408325195\tspeed (wps):2211.187432955872\n",
            "step: 406\tloss (sum over all examples' seen this epoch):84386.99712514877\tspeed (wps):2211.8506274936744\n",
            "step: 538\tloss (sum over all examples' seen this epoch):111789.97106075287\tspeed (wps):2212.302673728937\n",
            "step: 670\tloss (sum over all examples' seen this epoch):139259.2020368576\tspeed (wps):2212.696207098951\n",
            "step: 802\tloss (sum over all examples' seen this epoch):166282.6420354843\tspeed (wps):2213.3004040296178\n",
            "step: 934\tloss (sum over all examples' seen this epoch):193461.19588136673\tspeed (wps):2213.2757475866265\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):220633.71531009674\tspeed (wps):2213.186854772468\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):247543.86064291\tspeed (wps):2213.0873299901423\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 3\ttrain ppl: 362.28338963696865\tval ppl: 433.3845427738962\tbest val: 433.3845427738962\ttime (s) spent in epoch: 428.2422876358032\n",
            "\n",
            "EPOCH 4 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2385.035696029663\tspeed (wps):2175.7444090834374\n",
            "step: 142\tloss (sum over all examples' seen this epoch):29096.079943180084\tspeed (wps):2209.638795144765\n",
            "step: 274\tloss (sum over all examples' seen this epoch):56296.93284749985\tspeed (wps):2211.8500767544792\n",
            "step: 406\tloss (sum over all examples' seen this epoch):83245.66142320633\tspeed (wps):2212.0918745197323\n",
            "step: 538\tloss (sum over all examples' seen this epoch):110307.20014095306\tspeed (wps):2213.127268442449\n",
            "step: 670\tloss (sum over all examples' seen this epoch):137412.1472287178\tspeed (wps):2213.0064594956584\n",
            "step: 802\tloss (sum over all examples' seen this epoch):164101.9994020462\tspeed (wps):2213.1293485223273\n",
            "step: 934\tloss (sum over all examples' seen this epoch):190924.47373628616\tspeed (wps):2213.132247023492\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):217746.9035601616\tspeed (wps):2213.2795463307934\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):244325.83450317383\tspeed (wps):2213.4511987743\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 4\ttrain ppl: 335.9202770883028\tval ppl: 407.94834458047745\tbest val: 407.94834458047745\ttime (s) spent in epoch: 428.10559368133545\n",
            "\n",
            "EPOCH 5 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2319.74680185318\tspeed (wps):2165.7655625329558\n",
            "step: 142\tloss (sum over all examples' seen this epoch):28720.383894443512\tspeed (wps):2213.6472427869644\n",
            "step: 274\tloss (sum over all examples' seen this epoch):55651.653559207916\tspeed (wps):2213.407570454698\n",
            "step: 406\tloss (sum over all examples' seen this epoch):82296.14060878754\tspeed (wps):2213.301695309227\n",
            "step: 538\tloss (sum over all examples' seen this epoch):109036.59110307693\tspeed (wps):2213.3179856790625\n",
            "step: 670\tloss (sum over all examples' seen this epoch):135865.72615385056\tspeed (wps):2213.296200069377\n",
            "step: 802\tloss (sum over all examples' seen this epoch):162252.21370697021\tspeed (wps):2213.373190265064\n",
            "step: 934\tloss (sum over all examples' seen this epoch):188797.7902984619\tspeed (wps):2213.2424326282026\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):215387.42018461227\tspeed (wps):2213.639888146661\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):241719.85992193222\tspeed (wps):2213.437893676949\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 5\ttrain ppl: 315.7794082635882\tval ppl: 370.36007026185723\tbest val: 370.36007026185723\ttime (s) spent in epoch: 428.16359663009644\n",
            "\n",
            "EPOCH 6 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2272.483503818512\tspeed (wps):2176.353019417208\n",
            "step: 142\tloss (sum over all examples' seen this epoch):28413.491156101227\tspeed (wps):2208.387879773306\n",
            "step: 274\tloss (sum over all examples' seen this epoch):55100.55898666382\tspeed (wps):2211.0164952859313\n",
            "step: 406\tloss (sum over all examples' seen this epoch):81475.52533149719\tspeed (wps):2212.332445108368\n",
            "step: 538\tloss (sum over all examples' seen this epoch):107975.06755828857\tspeed (wps):2212.6261868452116\n",
            "step: 670\tloss (sum over all examples' seen this epoch):134548.85165452957\tspeed (wps):2213.6797484880203\n",
            "step: 802\tloss (sum over all examples' seen this epoch):160677.87436962128\tspeed (wps):2213.5680882833512\n",
            "step: 934\tloss (sum over all examples' seen this epoch):187018.9797091484\tspeed (wps):2213.5858425054507\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):213348.9908528328\tspeed (wps):2213.77084268459\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):239424.35863494873\tspeed (wps):2213.934202756091\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 6\ttrain ppl: 299.2522054752641\tval ppl: 346.753110838219\tbest val: 346.753110838219\ttime (s) spent in epoch: 428.05726170539856\n",
            "\n",
            "EPOCH 7 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2250.5942845344543\tspeed (wps):2180.015669638075\n",
            "step: 142\tloss (sum over all examples' seen this epoch):28170.205447673798\tspeed (wps):2210.2188822994667\n",
            "step: 274\tloss (sum over all examples' seen this epoch):54633.29633951187\tspeed (wps):2212.5224001844767\n",
            "step: 406\tloss (sum over all examples' seen this epoch):80785.83696603775\tspeed (wps):2214.1897900315917\n",
            "step: 538\tloss (sum over all examples' seen this epoch):107044.67091321945\tspeed (wps):2214.2224912782904\n",
            "step: 670\tloss (sum over all examples' seen this epoch):133390.99122524261\tspeed (wps):2214.3612351929214\n",
            "step: 802\tloss (sum over all examples' seen this epoch):159325.83436250687\tspeed (wps):2214.1248125800985\n",
            "step: 934\tloss (sum over all examples' seen this epoch):185460.42539596558\tspeed (wps):2214.0121446242147\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):211612.25395679474\tspeed (wps):2214.1181666282887\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):237438.2945036888\tspeed (wps):2213.9582713944587\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 7\ttrain ppl: 285.4113226987776\tval ppl: 341.5777394106408\tbest val: 341.5777394106408\ttime (s) spent in epoch: 427.99916315078735\n",
            "\n",
            "EPOCH 8 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2229.7970294952393\tspeed (wps):2163.9346775452555\n",
            "step: 142\tloss (sum over all examples' seen this epoch):27924.553413391113\tspeed (wps):2208.9824771769427\n",
            "step: 274\tloss (sum over all examples' seen this epoch):54199.97150182724\tspeed (wps):2211.640173059514\n",
            "step: 406\tloss (sum over all examples' seen this epoch):80137.77809143066\tspeed (wps):2212.1281813907285\n",
            "step: 538\tloss (sum over all examples' seen this epoch):106202.34467506409\tspeed (wps):2212.2128056617494\n",
            "step: 670\tloss (sum over all examples' seen this epoch):132359.6250462532\tspeed (wps):2212.880473437826\n",
            "step: 802\tloss (sum over all examples' seen this epoch):158060.00881195068\tspeed (wps):2212.8184700565944\n",
            "step: 934\tloss (sum over all examples' seen this epoch):183994.5712351799\tspeed (wps):2213.368618997079\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):209944.08365011215\tspeed (wps):2213.245489724419\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):235575.7622885704\tspeed (wps):2213.234015849908\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 8\ttrain ppl: 273.0520278671154\tval ppl: 329.3964788812583\tbest val: 329.3964788812583\ttime (s) spent in epoch: 428.220840215683\n",
            "\n",
            "EPOCH 9 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2218.0323910713196\tspeed (wps):2176.6497505719612\n",
            "step: 142\tloss (sum over all examples' seen this epoch):27728.352227211\tspeed (wps):2210.2980398983777\n",
            "step: 274\tloss (sum over all examples' seen this epoch):53850.5575799942\tspeed (wps):2212.396994961996\n",
            "step: 406\tloss (sum over all examples' seen this epoch):79621.77649497986\tspeed (wps):2212.301984384121\n",
            "step: 538\tloss (sum over all examples' seen this epoch):105528.51046085358\tspeed (wps):2213.5876304685507\n",
            "step: 670\tloss (sum over all examples' seen this epoch):131500.97266674042\tspeed (wps):2213.4534611769554\n",
            "step: 802\tloss (sum over all examples' seen this epoch):157035.814909935\tspeed (wps):2213.40323164487\n",
            "step: 934\tloss (sum over all examples' seen this epoch):182806.34484052658\tspeed (wps):2213.4472311009044\n",
            "step: 1066\tloss (sum over all examples' seen this epoch):208591.96957588196\tspeed (wps):2213.441043328499\n",
            "step: 1198\tloss (sum over all examples' seen this epoch):234088.12904119492\tspeed (wps):2213.6161395930385\n",
            "Saving model parameters to best_params.pt\n",
            "epoch: 9\ttrain ppl: 263.5588942163265\tval ppl: 281.5967060867704\tbest val: 281.5967060867704\ttime (s) spent in epoch: 428.1084372997284\n",
            "\n",
            "EPOCH 10 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):2199.116587638855\tspeed (wps):2174.287240027177\n",
            "step: 142\tloss (sum over all examples' seen this epoch):27556.996686458588\tspeed (wps):2211.652858848117\n",
            "step: 274\tloss (sum over all examples' seen this epoch):53504.38500404358\tspeed (wps):2213.8226805901595\n",
            "step: 406\tloss (sum over all examples' seen this epoch):79100.32510757446\tspeed (wps):2213.762277638721\n",
            "step: 538\tloss (sum over all examples' seen this epoch):104846.23701810837\tspeed (wps):2213.551112998006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FusPEXXAo3F9",
        "colab_type": "code",
        "outputId": "efcc1d48-9f81-4ca6-eab6-a99fee73941b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t   ptb.char.test.txt   ptb.char.valid.txt  ptb.test.txt   ptb.valid.txt\n",
            "models.py  ptb.char.train.txt  ptb-lm.py\t   ptb.train.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1toJGgwQqNx8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('ptb-lm.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ib0EZ89eQyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}